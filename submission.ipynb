{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec6f3253",
   "metadata": {},
   "source": [
    "**برای اجرای کد به انتهای فایل بروید**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dd86aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Callable, Dict, List, Tuple, TypeVar\n",
    "import sys\n",
    "sys.path.insert(1, r\"sentiment-main\")\n",
    "from util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604060d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "FeatureVector = Dict[str, int]\n",
    "WeightVector = Dict[str, float]\n",
    "Example = Tuple[FeatureVector, int]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0547d57a",
   "metadata": {},
   "source": [
    "### Problem 3: binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf12c849",
   "metadata": {},
   "source": [
    "##### Problem 3a: feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb76e982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractWordFeatures(x: str) -> FeatureVector:\n",
    "    \"\"\"\n",
    "    Extract word features for a string x. Words are delimited by\n",
    "    whitespace characters only.\n",
    "    @param string x:\n",
    "    @return dict: feature vector representation of x.\n",
    "    Example: \"I am what I am\" --> {'I': 2, 'am': 2, 'what': 1}\n",
    "    \"\"\"\n",
    "    # BEGIN_YOUR_CODE (our solution is 4 lines of code, but don't worry if you deviate from this)\n",
    "    words = x.split()  # Split the string into a list of words\n",
    "    featureVector  = {}  # Initialize an empty dictionary to store the feature vector\n",
    "    for word in words:  # Iterate over each word in the list\n",
    "        featureVector[word] = featureVector.get(word, 0) + 1  # Increment the count of the word in the feature vector\n",
    "    return featureVector  # Return the feature vector\n",
    "    # END_YOUR_CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410dd239",
   "metadata": {},
   "source": [
    "##### Problem 3b: stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5a42ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learnPredictor(trainExamples: List[Tuple[T, int]],\n",
    "                   validationExamples: List[Tuple[T, int]],\n",
    "                   featureExtractor: Callable[[T], FeatureVector],\n",
    "                   numEpochs: int, eta: float) -> WeightVector:\n",
    "    '''\n",
    "    Given |trainExamples| and |validationExamples| (each one is a list of (x,y)\n",
    "    pairs), a |featureExtractor| to apply to x, and the number of epochs to\n",
    "    train |numEpochs|, the step size |eta|, return the weight vector (sparse\n",
    "    feature vector) learned.\n",
    "\n",
    "    You should implement stochastic gradient descent.\n",
    "\n",
    "    Notes:\n",
    "    - Only use the trainExamples for training!\n",
    "    - You should call evaluatePredictor() on both trainExamples and validationExamples\n",
    "    to see how you're doing as you learn after each epoch.\n",
    "    - The predictor should output +1 if the score is precisely 0.\n",
    "    '''\n",
    "    weights = {}  # feature => weight\n",
    "\n",
    "    # BEGIN_YOUR_CODE (our solution is 13 lines of code, but don't worry if you deviate from this)\n",
    "    for epoch in range(numEpochs):\n",
    "        for example, label in trainExamples:\n",
    "            feature = featureExtractor(example)\n",
    "            margin = dotProduct(weights, feature) * label\n",
    "            if margin < 1:\n",
    "                # Update the weights using stochastic gradient descent\n",
    "                increment(weights, eta * label, feature)\n",
    "        # Create a predictor function based on the current weights\n",
    "        predictor = lambda example: 1 if dotProduct(featureExtractor(example), weights) >= 0 else -1\n",
    "        # Evaluate the training error using the predictor function\n",
    "        errorInTraining = evaluatePredictor(trainExamples, predictor)\n",
    "        # Evaluate the validation error using the predictor function\n",
    "        errorInValidation = evaluatePredictor(validationExamples, predictor)\n",
    "        # Print the epoch number and the corresponding errors\n",
    "        print(f\"Epoch {epoch + 1}/{numEpochs}: Train Error = {errorInTraining:.2f}, Validation Error = {errorInValidation:.2f}\")\n",
    "    # END_YOUR_CODE\n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0821b7",
   "metadata": {},
   "source": [
    "##### Problem 3c: generate test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5f905e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateDataset(numExamples: int, weights: WeightVector) -> List[Example]:\n",
    "    '''\n",
    "    Return a set of examples (phi(x), y) randomly which are classified correctly by\n",
    "    |weights|.\n",
    "    '''\n",
    "    random.seed(42)\n",
    "\n",
    "    # Return a single example (phi(x), y).\n",
    "    # phi(x) should be a dict whose keys are a subset of the keys in weights\n",
    "    # and values can be anything (randomize!) with a score for the given weight vector.\n",
    "    # note that there is intentionally flexibility in how you define phi.\n",
    "    # y should be 1 or -1 as classified by the weight vector.\n",
    "    # y should be 1 if the score is precisely 0.pytho\n",
    "\n",
    "    # Note that the weight vector can be arbitrary during testing.\n",
    "    def generateExample() -> Tuple[Dict[str, int], int]:\n",
    "        # BEGIN_YOUR_CODE (our solution is 3 lines of code, but don't worry if you deviate from this)\n",
    "        # Create an empty dictionary to store the feature vector phi\n",
    "        phi = {feature: random.choice([0, 1]) for feature in weights.keys()}  # Randomly assign values (0 or 1) to each feature in phi\n",
    "        y = 1 if sum(weights[f] * phi[f] for f in phi) >= 0 else -1  # Calculate the score for the given weight vector and classify y as 1 or -1\n",
    "        # END_YOUR_CODE\n",
    "        return phi, y\n",
    "\n",
    "    return [generateExample() for _ in range(numExamples)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af64d8a",
   "metadata": {},
   "source": [
    "##### Problem 3d: character features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "73ab2595",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractCharacterFeatures(n: int) -> Callable[[str], FeatureVector]:\n",
    "    '''\n",
    "    Return a function that takes a string |x| and returns a sparse feature\n",
    "    vector consisting of all n-grams of |x| without spaces mapped to their n-gram counts.\n",
    "    EXAMPLE: (n = 3) \"I like tacos\" --> {'Ili': 1, 'lik': 1, 'ike': 1, ...\n",
    "    You may assume that n >= 1.\n",
    "    '''\n",
    "    def extract(x: str) -> Dict[str, int]:\n",
    "        # BEGIN_YOUR_CODE (our solution is 6 lines of code, but don't worry if you deviate from this)\n",
    "        # Remove spaces from the input string and convert it to lowercase\n",
    "        x_no_spaces = x.replace(' ', '').lower()\n",
    "        # Initialize an empty dictionary to store the character sequences and their counts\n",
    "        character_sequence = {}\n",
    "        # Iterate over each substring of length n in the input string\n",
    "        for i in range(len(x_no_spaces) - n + 1):\n",
    "            # Extract the current substring\n",
    "            substring = x_no_spaces[i:i+n]\n",
    "            # Check if the substring is already in the dictionary\n",
    "            if substring in character_sequence:\n",
    "                # If it is, increment its count by 1\n",
    "                character_sequence[substring] = character_sequence[substring] + 1\n",
    "            else:\n",
    "                # If it is not, add it to the dictionary with a count of 1\n",
    "                character_sequence[substring] = 1\n",
    "        # Return the dictionary containing the character sequences and their counts\n",
    "        return character_sequence\n",
    "        # END_YOUR_CODE\n",
    "\n",
    "    return extract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc61ed3",
   "metadata": {},
   "source": [
    "##### Problem 3e:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4a8616",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testValuesOfN(n: int):\n",
    "    '''\n",
    "    Use this code to test different values of n for extractCharacterFeatures\n",
    "    This code is exclusively for testing.\n",
    "    Your full written solution for this problem must be in sentiment.pdf.\n",
    "    '''\n",
    "    trainExamples = readExamples(r\"polarity.train\")\n",
    "    validationExamples = readExamples(r\"polarity.train\")\n",
    "    featureExtractor = extractCharacterFeatures(n)\n",
    "    weights = learnPredictor(trainExamples,\n",
    "                             validationExamples,\n",
    "                             featureExtractor,\n",
    "                             numEpochs=20,\n",
    "                             eta=0.01)\n",
    "    outputWeights(weights, 'weights')\n",
    "    outputErrorAnalysis(validationExamples, featureExtractor, weights,\n",
    "                        'error-analysis')  # Use this to debug\n",
    "    trainError = evaluatePredictor(\n",
    "        trainExamples, lambda x:\n",
    "        (1 if dotProduct(featureExtractor(x), weights) >= 0 else -1))\n",
    "    validationError = evaluatePredictor(\n",
    "        validationExamples, lambda x:\n",
    "        (1 if dotProduct(featureExtractor(x), weights) >= 0 else -1))\n",
    "    print((\"Official: train error = %s, validation error = %s\" %\n",
    "           (trainError, validationError)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a809c1",
   "metadata": {},
   "source": [
    "**Question:**\n",
    "Experiment with different values of n to see which one produces the smallest validation error. You should observe that this error is nearly as small as that produced by word features. Why is this the case?\n",
    "\n",
    "**Answer:**\n",
    "The reason why character features (n-grams) can produce nearly as small validation error as word features is because n-grams can capture the local structure of the language. Imagine a 3-gram can capture the structure of a three-letter word or a three-word phrase. This local structure can be very informative for many tasks, such as text classification or sentiment analysis.\n",
    "For example: \"The food was not good, it was fantastic!\"\n",
    "In this case, character n-grams might outperform word features because they can capture the negation \"not good\" and the contrast indicated by \"it was fantastic\". Word features might treat \"not\", \"good\", and \"fantastic\" as independent features and miss the overall sentiment of the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed04b1e",
   "metadata": {},
   "source": [
    "##### Test Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a141f3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== START GRADING\n",
      "----- START PART 3a-0-basic: basic test\n",
      "----- END PART 3a-0-basic [took 0:00:00 (max allowed 1 seconds), 1/1 points]\n",
      "\n",
      "----- START PART 3a-1-hidden: test multiple instances of the same word in a sentence\n",
      "----- END PART 3a-1-hidden [took 0:00:00 (max allowed 1 seconds), ???/1 points (hidden test ungraded)]\n",
      "\n",
      "----- START PART 3b-0-basic: basic sanity check for learning correct weights on two training and testing examples each\n",
      "Epoch 1/20: Train Error = 0.00, Validation Error = 0.00\n",
      "Epoch 2/20: Train Error = 0.00, Validation Error = 0.00\n",
      "Epoch 3/20: Train Error = 0.00, Validation Error = 0.00\n",
      "Epoch 4/20: Train Error = 0.00, Validation Error = 0.00\n",
      "Epoch 5/20: Train Error = 0.00, Validation Error = 0.00\n",
      "Epoch 6/20: Train Error = 0.00, Validation Error = 0.00\n",
      "Epoch 7/20: Train Error = 0.00, Validation Error = 0.00\n",
      "Epoch 8/20: Train Error = 0.00, Validation Error = 0.00\n",
      "Epoch 9/20: Train Error = 0.00, Validation Error = 0.00\n",
      "Epoch 10/20: Train Error = 0.00, Validation Error = 0.00\n",
      "Epoch 11/20: Train Error = 0.00, Validation Error = 0.00\n",
      "Epoch 12/20: Train Error = 0.00, Validation Error = 0.00\n",
      "Epoch 13/20: Train Error = 0.00, Validation Error = 0.00\n",
      "Epoch 14/20: Train Error = 0.00, Validation Error = 0.00\n",
      "Epoch 15/20: Train Error = 0.00, Validation Error = 0.00\n",
      "Epoch 16/20: Train Error = 0.00, Validation Error = 0.00\n",
      "Epoch 17/20: Train Error = 0.00, Validation Error = 0.00\n",
      "Epoch 18/20: Train Error = 0.00, Validation Error = 0.00\n",
      "Epoch 19/20: Train Error = 0.00, Validation Error = 0.00\n",
      "Epoch 20/20: Train Error = 0.00, Validation Error = 0.00\n",
      "----- END PART 3b-0-basic [took 0:00:00 (max allowed 1 seconds), 1/1 points]\n",
      "\n",
      "----- START PART 3b-1-basic: test correct overriding of positive weight due to one negative instance with repeated words\n",
      "Epoch 1/20: Train Error = 0.00, Validation Error = 0.00\n",
      "Epoch 2/20: Train Error = 0.00, Validation Error = 0.00\n",
      "Epoch 3/20: Train Error = 0.00, Validation Error = 0.00\n",
      "Epoch 4/20: Train Error = 0.00, Validation Error = 0.00\n",
      "Epoch 5/20: Train Error = 0.00, Validation Error = 0.00\n",
      "Epoch 6/20: Train Error = 0.00, Validation Error = 0.00\n",
      "Epoch 7/20: Train Error = 0.00, Validation Error = 0.00\n",
      "Epoch 8/20: Train Error = 0.00, Validation Error = 0.00\n",
      "Epoch 9/20: Train Error = 0.00, Validation Error = 0.00\n",
      "Epoch 10/20: Train Error = 0.00, Validation Error = 0.00\n",
      "Epoch 11/20: Train Error = 0.50, Validation Error = 0.00\n",
      "Epoch 12/20: Train Error = 0.50, Validation Error = 0.00\n",
      "Epoch 13/20: Train Error = 0.50, Validation Error = 0.00\n",
      "Epoch 14/20: Train Error = 0.50, Validation Error = 0.00\n",
      "Epoch 15/20: Train Error = 0.00, Validation Error = 0.00\n",
      "Epoch 16/20: Train Error = 0.00, Validation Error = 0.00\n",
      "Epoch 17/20: Train Error = 0.00, Validation Error = 0.00\n",
      "Epoch 18/20: Train Error = 0.00, Validation Error = 0.00\n",
      "Epoch 19/20: Train Error = 0.00, Validation Error = 0.00\n",
      "Epoch 20/20: Train Error = 0.00, Validation Error = 0.00\n",
      "----- END PART 3b-1-basic [took 0:00:00.000999 (max allowed 2 seconds), 1/1 points]\n",
      "\n",
      "----- START PART 3b-2-basic: test classifier on real polarity dev dataset\n",
      "Read 3554 examples from polarity.train\n",
      "Read 3554 examples from polarity.dev\n",
      "Epoch 1/20: Train Error = 0.29, Validation Error = 0.35\n",
      "Epoch 2/20: Train Error = 0.22, Validation Error = 0.32\n",
      "Epoch 3/20: Train Error = 0.18, Validation Error = 0.31\n",
      "Epoch 4/20: Train Error = 0.15, Validation Error = 0.30\n",
      "Epoch 5/20: Train Error = 0.13, Validation Error = 0.29\n",
      "Epoch 6/20: Train Error = 0.11, Validation Error = 0.29\n",
      "Epoch 7/20: Train Error = 0.10, Validation Error = 0.28\n",
      "Epoch 8/20: Train Error = 0.09, Validation Error = 0.28\n",
      "Epoch 9/20: Train Error = 0.08, Validation Error = 0.28\n",
      "Epoch 10/20: Train Error = 0.07, Validation Error = 0.28\n",
      "Epoch 11/20: Train Error = 0.06, Validation Error = 0.28\n",
      "Epoch 12/20: Train Error = 0.06, Validation Error = 0.28\n",
      "Epoch 13/20: Train Error = 0.05, Validation Error = 0.27\n",
      "Epoch 14/20: Train Error = 0.05, Validation Error = 0.28\n",
      "Epoch 15/20: Train Error = 0.04, Validation Error = 0.28\n",
      "Epoch 16/20: Train Error = 0.04, Validation Error = 0.27\n",
      "Epoch 17/20: Train Error = 0.04, Validation Error = 0.27\n",
      "Epoch 18/20: Train Error = 0.03, Validation Error = 0.27\n",
      "Epoch 19/20: Train Error = 0.03, Validation Error = 0.28\n",
      "Epoch 20/20: Train Error = 0.03, Validation Error = 0.28\n",
      "11211 weights\n",
      "Official: train error = 0.027011817670230726, validation error = 0.2751828925154755\n",
      "----- END PART 3b-2-basic [took 0:00:01.746852 (max allowed 16 seconds), 2/2 points]\n",
      "\n",
      "----- START PART 3c-0-basic: test correct generation of small dataset labels\n",
      "----- END PART 3c-0-basic [took 0:00:00 (max allowed 2 seconds), 1/1 points]\n",
      "\n",
      "----- START PART 3c-1-basic: test correct generation of large random dataset labels\n",
      "----- END PART 3c-1-basic [took 0:00:00.007330 (max allowed 2 seconds), 1/1 points]\n",
      "\n",
      "----- START PART 3d-0-basic: test basic character n-gram features\n",
      "----- END PART 3d-0-basic [took 0:00:00 (max allowed 1 seconds), 1/1 points]\n",
      "\n",
      "----- START PART 3d-1-hidden: test feature extraction on repeated character n-grams\n",
      "----- END PART 3d-1-hidden [took 0:00:00.001019 (max allowed 2 seconds), ???/1 points (hidden test ungraded)]\n",
      "\n",
      "Note that the hidden test cases do not check for correctness.\n",
      "They are provided for you to verify that the functions do not crash and run within the time limit.\n",
      "Points for these parts not assigned by the grader (indicated by \"--\").\n",
      "========== END GRADING [8/8 points + 0/0 extra credit]\n"
     ]
    }
   ],
   "source": [
    "!python ./grader.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
